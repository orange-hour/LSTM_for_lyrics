{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fwX2v1vbyxGD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m5A9o4SyycD",
        "outputId": "5941bab3-0560-4b5b-d2aa-82340b534c94"
      },
      "outputs": [],
      "source": [
        "#konlpy 설치\n",
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CaTUFbQsr23N"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab, Okt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6QVr4fH2x6nG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def text_preprocessing(text_list):   #list 형태의 text_list를 입력받는 전처리 함수\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', '.', '\"', '!', '~', '게', '걸', '았'] #불용어 설정\n",
        "    tokenizer = Mecab() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for lyrics in text_list:   #바깥쪽 리스트\n",
        "\n",
        "        for text in lyrics:    #안쪽 리스트\n",
        "            txt = re.sub('[^가-힣]', ' ', text) #한글만 남기고 다른 글자 모두 제거\n",
        "            token = tokenizer.morphs(txt) #형태소 분석\n",
        "            token = [t for t in token if t not in stopwords] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "            token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_G6OPH1XxCH",
        "outputId": "f3488779-d0c2-4aa1-ee1c-3ea6223d98eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B_BuYmkyW-jn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "lyrics_dir = \"/content/drive/MyDrive/sample_data/nell_lyrics/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mKl0685QYMeV"
      },
      "outputs": [],
      "source": [
        "filenames = os.listdir(lyrics_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_idq3g90YfX0"
      },
      "outputs": [],
      "source": [
        "def text_load(base_dir, name):\n",
        "    text_path = base_dir\n",
        "    f = open(base_dir + name, 'r')\n",
        "\n",
        "    lines = f.readlines()\n",
        "    lines = [line.strip() for line in lines]\n",
        "    \n",
        "    f.close()\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pCQbPzFhW4WK"
      },
      "outputs": [],
      "source": [
        "all_lyrics = []\n",
        "\n",
        "for filename in filenames:\n",
        "    all_lyrics.append(text_load(lyrics_dir, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYzpOLZ1bsTm",
        "outputId": "ede49249-984e-4154-dddf-5f881d6d7389"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['지금까진 전부 잊어',\n",
              "  '조용히 두 눈을 감고, 널 가둔 그 벽을 부숴',\n",
              "  '알고 있었던 모든게 아무것도 아닌게 될까,',\n",
              "  '그렇게 될까봐 두려워?',\n",
              "  'Just breathe in & breathe out your dreams with me.',\n",
              "  '',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 숨을 쉰다',\n",
              "  '눈부신 빛의 파도 그 속에서 새롭게 태어나고 있어',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 춤을 춘다',\n",
              "  '거대한 빛의 파도 그 속에서 난 다시 태어나고 있어',\n",
              "  'In the ocean of light',\n",
              "  '',\n",
              "  '새로워질 너를 믿어',\n",
              "  '조용히 두 눈을 감고, 늘 꿈꿔왔던 너를 그려',\n",
              "  '믿어왔었던 모든 게 아무것도 아닌 게 될까,',\n",
              "  '그렇게 될까봐 두려워?',\n",
              "  'Just breathe in & breathe out your dreams with me',\n",
              "  '',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 숨을 쉰다',\n",
              "  '눈부신 빛의 파도 그 속에서 새롭게 태어나고 있어',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 춤을 춘다',\n",
              "  '거대한 빛의 파도 그 속에서 난 다시 태어나고 있어',\n",
              "  'In the ocean of light',\n",
              "  '',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 숨을 쉰다',\n",
              "  '눈부신 빛의 파도 그 속에서 새롭게 태어나고 있어',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 춤을 춘다',\n",
              "  '거대한 빛의 파도 그 속에서',\n",
              "  '난 다시 태어나고 있어',\n",
              "  'In the ocean of light',\n",
              "  '',\n",
              "  'In the ocean of light',\n",
              "  'In the ocean of light',\n",
              "  'In the ocean of light',\n",
              "  \"I'm in the ocean of light\"],\n",
              " ['하루가 길었어',\n",
              "  '고작 글자 몇 개',\n",
              "  '난 또 그렇게 망가져 버려',\n",
              "  '',\n",
              "  '애써 거릴 두며',\n",
              "  '부여잡고 있던',\n",
              "  '마음이 다 흐트러져버려',\n",
              "  '',\n",
              "  '정신 차려 보니',\n",
              "  '이미 넌 내 앞에',\n",
              "  '밀어내려 했던 노력 따윈',\n",
              "  '의미 없는 거지',\n",
              "  '',\n",
              "  '제발 오늘만큼은',\n",
              "  '여기 있어 줘',\n",
              "  '그냥 아무 말 없이',\n",
              "  '나를 안아줘',\n",
              "  '',\n",
              "  '이렇게 네가 내',\n",
              "  '앞에 서 있으면',\n",
              "  '이 모든 건 다 헛수고가 돼',\n",
              "  '',\n",
              "  '그저 어떻게든',\n",
              "  '고쳐보려 했던',\n",
              "  '내 맘이 다시 고장 나 버려',\n",
              "  '',\n",
              "  '서툰 듯 보여도',\n",
              "  '꽤나 능숙하지',\n",
              "  '말은 안 했어도',\n",
              "  '이미 넌 내 전부인 것처럼',\n",
              "  '',\n",
              "  '제발 오늘만큼은',\n",
              "  '여기 있어 줘',\n",
              "  '그냥 아무 말 없이',\n",
              "  '나를 안아줘',\n",
              "  '',\n",
              "  '제발 이 순간만은',\n",
              "  '내 것이어 줘',\n",
              "  '그냥 아무 말 없이',\n",
              "  '입을 맞춰줘',\n",
              "  '',\n",
              "  '시들해진 마음속에 나를 담아',\n",
              "  '하룻밤의 유희라도 난 괜찮아',\n",
              "  '슬프지만 아름다워 그대잖아',\n",
              "  '깨져버릴 꿈이라도 난 괜찮아',\n",
              "  '',\n",
              "  '시들해진 마음속에 나를 담아',\n",
              "  '하룻밤의 유희라도 난 괜찮아',\n",
              "  '슬프지만 아름다워 그대잖아',\n",
              "  '깨져버릴 꿈이라도 난 괜찮아',\n",
              "  '',\n",
              "  '시들해진 마음속에 나를 담아']]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_lyrics[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HGsloQSGhJhO"
      },
      "outputs": [],
      "source": [
        "#리스트 내부 빈 문자열 제거\n",
        "all_lyrics = [list(filter(None, lyrics)) for lyrics in all_lyrics]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGj1e1o7hWyW",
        "outputId": "8a8ca358-3204-40b8-ca4b-66ed51d957db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['지금까진 전부 잊어',\n",
              "  '조용히 두 눈을 감고, 널 가둔 그 벽을 부숴',\n",
              "  '알고 있었던 모든게 아무것도 아닌게 될까,',\n",
              "  '그렇게 될까봐 두려워?',\n",
              "  'Just breathe in & breathe out your dreams with me.',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 숨을 쉰다',\n",
              "  '눈부신 빛의 파도 그 속에서 새롭게 태어나고 있어',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 춤을 춘다',\n",
              "  '거대한 빛의 파도 그 속에서 난 다시 태어나고 있어',\n",
              "  'In the ocean of light',\n",
              "  '새로워질 너를 믿어',\n",
              "  '조용히 두 눈을 감고, 늘 꿈꿔왔던 너를 그려',\n",
              "  '믿어왔었던 모든 게 아무것도 아닌 게 될까,',\n",
              "  '그렇게 될까봐 두려워?',\n",
              "  'Just breathe in & breathe out your dreams with me',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 숨을 쉰다',\n",
              "  '눈부신 빛의 파도 그 속에서 새롭게 태어나고 있어',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 춤을 춘다',\n",
              "  '거대한 빛의 파도 그 속에서 난 다시 태어나고 있어',\n",
              "  'In the ocean of light',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 숨을 쉰다',\n",
              "  '눈부신 빛의 파도 그 속에서 새롭게 태어나고 있어',\n",
              "  'I’m in the ocean of light',\n",
              "  '내 꿈이 춤을 춘다',\n",
              "  '거대한 빛의 파도 그 속에서',\n",
              "  '난 다시 태어나고 있어',\n",
              "  'In the ocean of light',\n",
              "  'In the ocean of light',\n",
              "  'In the ocean of light',\n",
              "  'In the ocean of light',\n",
              "  \"I'm in the ocean of light\"],\n",
              " ['하루가 길었어',\n",
              "  '고작 글자 몇 개',\n",
              "  '난 또 그렇게 망가져 버려',\n",
              "  '애써 거릴 두며',\n",
              "  '부여잡고 있던',\n",
              "  '마음이 다 흐트러져버려',\n",
              "  '정신 차려 보니',\n",
              "  '이미 넌 내 앞에',\n",
              "  '밀어내려 했던 노력 따윈',\n",
              "  '의미 없는 거지',\n",
              "  '제발 오늘만큼은',\n",
              "  '여기 있어 줘',\n",
              "  '그냥 아무 말 없이',\n",
              "  '나를 안아줘',\n",
              "  '이렇게 네가 내',\n",
              "  '앞에 서 있으면',\n",
              "  '이 모든 건 다 헛수고가 돼',\n",
              "  '그저 어떻게든',\n",
              "  '고쳐보려 했던',\n",
              "  '내 맘이 다시 고장 나 버려',\n",
              "  '서툰 듯 보여도',\n",
              "  '꽤나 능숙하지',\n",
              "  '말은 안 했어도',\n",
              "  '이미 넌 내 전부인 것처럼',\n",
              "  '제발 오늘만큼은',\n",
              "  '여기 있어 줘',\n",
              "  '그냥 아무 말 없이',\n",
              "  '나를 안아줘',\n",
              "  '제발 이 순간만은',\n",
              "  '내 것이어 줘',\n",
              "  '그냥 아무 말 없이',\n",
              "  '입을 맞춰줘',\n",
              "  '시들해진 마음속에 나를 담아',\n",
              "  '하룻밤의 유희라도 난 괜찮아',\n",
              "  '슬프지만 아름다워 그대잖아',\n",
              "  '깨져버릴 꿈이라도 난 괜찮아',\n",
              "  '시들해진 마음속에 나를 담아',\n",
              "  '하룻밤의 유희라도 난 괜찮아',\n",
              "  '슬프지만 아름다워 그대잖아',\n",
              "  '깨져버릴 꿈이라도 난 괜찮아',\n",
              "  '시들해진 마음속에 나를 담아']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_lyrics[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WYlQd_h-0K18"
      },
      "outputs": [],
      "source": [
        "#가사 전체 토큰화\n",
        "lyrics_tokenlist, mecab = text_preprocessing(all_lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5mvVDL2c-Mq",
        "outputId": "1a8588ec-837f-4fb0-ca25-8e5575c03050"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['지금', '까진', '전부', '잊', '어'],\n",
              " ['조용히', '두', '눈', '감', '고', '널', '가둔', '그', '벽', '부숴'],\n",
              " ['알', '고', '있', '었', '던', '모든', '아무것', '도', '아닌게', '될까'],\n",
              " ['그렇', '될까봐', '두려워'],\n",
              " [],\n",
              " [],\n",
              " ['내', '꿈', '숨', '쉰다'],\n",
              " ['눈부신', '빛', '의', '파도', '그', '속', '에서', '새롭', '태어나', '고', '있', '어'],\n",
              " [],\n",
              " ['내', '꿈', '춤', '춘다'],\n",
              " ['거대', '한', '빛', '의', '파도', '그', '속', '에서', '난', '다시', '태어나', '고', '있', '어'],\n",
              " [],\n",
              " ['새로워질', '너', '믿', '어'],\n",
              " ['조용히', '두', '눈', '감', '고', '늘', '꿈꿔왔', '던', '너', '그려'],\n",
              " ['믿', '어', '왔었', '던', '모든', '아무것', '도', '아닌', '될까'],\n",
              " ['그렇', '될까봐', '두려워'],\n",
              " [],\n",
              " [],\n",
              " ['내', '꿈', '숨', '쉰다'],\n",
              " ['눈부신', '빛', '의', '파도', '그', '속', '에서', '새롭', '태어나', '고', '있', '어'],\n",
              " [],\n",
              " ['내', '꿈', '춤', '춘다'],\n",
              " ['거대', '한', '빛', '의', '파도', '그', '속', '에서', '난', '다시', '태어나', '고', '있', '어'],\n",
              " [],\n",
              " [],\n",
              " ['내', '꿈', '숨', '쉰다'],\n",
              " ['눈부신', '빛', '의', '파도', '그', '속', '에서', '새롭', '태어나', '고', '있', '어'],\n",
              " [],\n",
              " ['내', '꿈', '춤', '춘다'],\n",
              " ['거대', '한', '빛', '의', '파도', '그', '속', '에서'],\n",
              " ['난', '다시', '태어나', '고', '있', '어'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['하루', '길', '었', '어'],\n",
              " ['고작', '글자', '몇', '개'],\n",
              " ['난', '또', '그렇게', '망가져', '버려'],\n",
              " ['애써', '거릴', '두', '며'],\n",
              " ['부여잡', '고', '있', '던'],\n",
              " ['마음', '다', '흐트러', '져버려'],\n",
              " ['정신', '차려', '보', '니'],\n",
              " ['이미', '넌', '내', '앞', '에'],\n",
              " ['밀어내', '려', '했', '던', '노력', '따윈'],\n",
              " ['의미', '없', '거', '지'],\n",
              " ['제발', '오늘', '만큼'],\n",
              " ['여기', '있', '어', '줘'],\n",
              " ['그냥', '아무', '말', '없이'],\n",
              " ['나', '안', '아', '줘']]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lyrics_tokenlist[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j-wscD83e21Z"
      },
      "outputs": [],
      "source": [
        "#토큰 리스트 내부 빈 리스트 제거\n",
        "lyrics_tokenlist = list(filter(None, lyrics_tokenlist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWW_jDUhjn4o",
        "outputId": "329c9a16-eac9-4996-99e3-6d5adba9873a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['지금', '까진', '전부', '잊', '어'],\n",
              " ['조용히', '두', '눈', '감', '고', '널', '가둔', '그', '벽', '부숴'],\n",
              " ['알', '고', '있', '었', '던', '모든', '아무것', '도', '아닌게', '될까'],\n",
              " ['그렇', '될까봐', '두려워'],\n",
              " ['내', '꿈', '숨', '쉰다'],\n",
              " ['눈부신', '빛', '의', '파도', '그', '속', '에서', '새롭', '태어나', '고', '있', '어'],\n",
              " ['내', '꿈', '춤', '춘다'],\n",
              " ['거대', '한', '빛', '의', '파도', '그', '속', '에서', '난', '다시', '태어나', '고', '있', '어'],\n",
              " ['새로워질', '너', '믿', '어'],\n",
              " ['조용히', '두', '눈', '감', '고', '늘', '꿈꿔왔', '던', '너', '그려'],\n",
              " ['믿', '어', '왔었', '던', '모든', '아무것', '도', '아닌', '될까'],\n",
              " ['그렇', '될까봐', '두려워'],\n",
              " ['내', '꿈', '숨', '쉰다'],\n",
              " ['눈부신', '빛', '의', '파도', '그', '속', '에서', '새롭', '태어나', '고', '있', '어'],\n",
              " ['내', '꿈', '춤', '춘다'],\n",
              " ['거대', '한', '빛', '의', '파도', '그', '속', '에서', '난', '다시', '태어나', '고', '있', '어'],\n",
              " ['내', '꿈', '숨', '쉰다'],\n",
              " ['눈부신', '빛', '의', '파도', '그', '속', '에서', '새롭', '태어나', '고', '있', '어'],\n",
              " ['내', '꿈', '춤', '춘다'],\n",
              " ['거대', '한', '빛', '의', '파도', '그', '속', '에서'],\n",
              " ['난', '다시', '태어나', '고', '있', '어'],\n",
              " ['하루', '길', '었', '어'],\n",
              " ['고작', '글자', '몇', '개'],\n",
              " ['난', '또', '그렇게', '망가져', '버려'],\n",
              " ['애써', '거릴', '두', '며'],\n",
              " ['부여잡', '고', '있', '던'],\n",
              " ['마음', '다', '흐트러', '져버려'],\n",
              " ['정신', '차려', '보', '니'],\n",
              " ['이미', '넌', '내', '앞', '에'],\n",
              " ['밀어내', '려', '했', '던', '노력', '따윈'],\n",
              " ['의미', '없', '거', '지'],\n",
              " ['제발', '오늘', '만큼'],\n",
              " ['여기', '있', '어', '줘'],\n",
              " ['그냥', '아무', '말', '없이'],\n",
              " ['나', '안', '아', '줘'],\n",
              " ['이렇게', '네', '내'],\n",
              " ['앞', '에', '서', '있', '으면'],\n",
              " ['모든', '건', '다', '헛', '수고', '돼'],\n",
              " ['그저', '어떻게', '든'],\n",
              " ['고쳐', '보', '려', '했', '던'],\n",
              " ['내', '맘', '다시', '고장', '나', '버려'],\n",
              " ['서툰', '듯', '보여도'],\n",
              " ['꽤', '나', '능숙', '하', '지'],\n",
              " ['말', '안', '했어도'],\n",
              " ['이미', '넌', '내', '전부', '인', '것', '처럼'],\n",
              " ['제발', '오늘', '만큼'],\n",
              " ['여기', '있', '어', '줘'],\n",
              " ['그냥', '아무', '말', '없이'],\n",
              " ['나', '안', '아', '줘'],\n",
              " ['제발', '순간', '만']]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lyrics_tokenlist[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlMrQHF07wbJ",
        "outputId": "329e2879-80c6-418d-d4e8-99903533d2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 2153\n"
          ]
        }
      ],
      "source": [
        "#vocab size 불러오기\n",
        "\n",
        "tokenizer_keras = Tokenizer()\n",
        "tokenizer_keras.fit_on_texts(lyrics_tokenlist)\n",
        "vocab_size = len(tokenizer_keras.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JwJ3l8L48Lbe"
      },
      "outputs": [],
      "source": [
        "sequences = list()\n",
        "\n",
        "for sentence in lyrics_tokenlist:\n",
        "\n",
        "    # 각 샘플에 대한 정수 인코딩\n",
        "    encoded = tokenizer_keras.texts_to_sequences([sentence])[0] \n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyI3pvtc8rmI",
        "outputId": "ddc6c989-d63a-40b9-f54c-79d6ccd82816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "빈도수 상위 5번 단어 : 해\n"
          ]
        }
      ],
      "source": [
        "index_to_word = {}\n",
        "for key, value in tokenizer_keras.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
        "    index_to_word[value] = key\n",
        "\n",
        "print('빈도수 상위 5번 단어 : {}'.format(index_to_word[5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUk4CRhv8wln",
        "outputId": "48883bac-6961-46fc-c844-a89e0f0cd952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 18\n"
          ]
        }
      ],
      "source": [
        "max_len = max(len(l) for l in sequences)\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96QEeDEr83L-",
        "outputId": "afada488-c114-4a36-acd6-545c2d91a5d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 608]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 608 232]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 608 232 107]]\n"
          ]
        }
      ],
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w-HRYxd586tP"
      },
      "outputs": [],
      "source": [
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1] #맨 우측 단어만 레이블로 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTd2yAuq9QVD",
        "outputId": "91a27851-da42-46b5-f951-9d3c5bfbb688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 608]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 608 232]]\n"
          ]
        }
      ],
      "source": [
        "print(X[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ya3_GMS_9Xqp"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)  #레이블 데이터 y에 대해서 원-핫 인코딩을 수행 (loss: categorical_crossentropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVgZmSMJ9enL",
        "outputId": "bfaa2026-6023-4ffa-bdc5-f8b7075a349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "O397OwlV-tkT"
      },
      "outputs": [],
      "source": [
        "#모델 설계\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRue3_16-6o5",
        "outputId": "12f765e2-d7fb-4544-d83b-4482aeeeda6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "486/486 - 8s - loss: 6.4812 - accuracy: 0.0256 - 8s/epoch - 17ms/step\n",
            "Epoch 2/300\n",
            "486/486 - 5s - loss: 6.0183 - accuracy: 0.0253 - 5s/epoch - 9ms/step\n",
            "Epoch 3/300\n",
            "486/486 - 4s - loss: 5.9085 - accuracy: 0.0362 - 4s/epoch - 9ms/step\n",
            "Epoch 4/300\n",
            "486/486 - 4s - loss: 5.7573 - accuracy: 0.0561 - 4s/epoch - 9ms/step\n",
            "Epoch 5/300\n",
            "486/486 - 4s - loss: 5.6033 - accuracy: 0.0720 - 4s/epoch - 9ms/step\n",
            "Epoch 6/300\n",
            "486/486 - 4s - loss: 5.4828 - accuracy: 0.0895 - 4s/epoch - 9ms/step\n",
            "Epoch 7/300\n",
            "486/486 - 4s - loss: 5.3874 - accuracy: 0.1003 - 4s/epoch - 9ms/step\n",
            "Epoch 8/300\n",
            "486/486 - 4s - loss: 5.3071 - accuracy: 0.1064 - 4s/epoch - 9ms/step\n",
            "Epoch 9/300\n",
            "486/486 - 4s - loss: 5.2394 - accuracy: 0.1104 - 4s/epoch - 9ms/step\n",
            "Epoch 10/300\n",
            "486/486 - 4s - loss: 5.1836 - accuracy: 0.1147 - 4s/epoch - 9ms/step\n",
            "Epoch 11/300\n",
            "486/486 - 5s - loss: 5.1380 - accuracy: 0.1219 - 5s/epoch - 9ms/step\n",
            "Epoch 12/300\n",
            "486/486 - 5s - loss: 5.1004 - accuracy: 0.1215 - 5s/epoch - 9ms/step\n",
            "Epoch 13/300\n",
            "486/486 - 5s - loss: 5.0657 - accuracy: 0.1250 - 5s/epoch - 9ms/step\n",
            "Epoch 14/300\n",
            "486/486 - 5s - loss: 5.0306 - accuracy: 0.1266 - 5s/epoch - 9ms/step\n",
            "Epoch 15/300\n",
            "486/486 - 4s - loss: 4.9991 - accuracy: 0.1290 - 4s/epoch - 9ms/step\n",
            "Epoch 16/300\n",
            "486/486 - 4s - loss: 4.9663 - accuracy: 0.1319 - 4s/epoch - 9ms/step\n",
            "Epoch 17/300\n",
            "486/486 - 5s - loss: 4.9372 - accuracy: 0.1322 - 5s/epoch - 9ms/step\n",
            "Epoch 18/300\n",
            "486/486 - 4s - loss: 4.9066 - accuracy: 0.1343 - 4s/epoch - 9ms/step\n",
            "Epoch 19/300\n",
            "486/486 - 4s - loss: 4.8680 - accuracy: 0.1365 - 4s/epoch - 9ms/step\n",
            "Epoch 20/300\n",
            "486/486 - 4s - loss: 4.8364 - accuracy: 0.1391 - 4s/epoch - 9ms/step\n",
            "Epoch 21/300\n",
            "486/486 - 4s - loss: 4.8029 - accuracy: 0.1400 - 4s/epoch - 9ms/step\n",
            "Epoch 22/300\n",
            "486/486 - 5s - loss: 4.7685 - accuracy: 0.1418 - 5s/epoch - 9ms/step\n",
            "Epoch 23/300\n",
            "486/486 - 4s - loss: 4.7353 - accuracy: 0.1449 - 4s/epoch - 9ms/step\n",
            "Epoch 24/300\n",
            "486/486 - 5s - loss: 4.7013 - accuracy: 0.1471 - 5s/epoch - 9ms/step\n",
            "Epoch 25/300\n",
            "486/486 - 4s - loss: 4.6730 - accuracy: 0.1498 - 4s/epoch - 9ms/step\n",
            "Epoch 26/300\n",
            "486/486 - 4s - loss: 4.6463 - accuracy: 0.1502 - 4s/epoch - 9ms/step\n",
            "Epoch 27/300\n",
            "486/486 - 4s - loss: 4.6198 - accuracy: 0.1520 - 4s/epoch - 9ms/step\n",
            "Epoch 28/300\n",
            "486/486 - 4s - loss: 4.5858 - accuracy: 0.1555 - 4s/epoch - 9ms/step\n",
            "Epoch 29/300\n",
            "486/486 - 5s - loss: 4.5593 - accuracy: 0.1559 - 5s/epoch - 9ms/step\n",
            "Epoch 30/300\n",
            "486/486 - 4s - loss: 4.5221 - accuracy: 0.1576 - 4s/epoch - 9ms/step\n",
            "Epoch 31/300\n",
            "486/486 - 4s - loss: 4.4922 - accuracy: 0.1629 - 4s/epoch - 9ms/step\n",
            "Epoch 32/300\n",
            "486/486 - 4s - loss: 4.4736 - accuracy: 0.1629 - 4s/epoch - 9ms/step\n",
            "Epoch 33/300\n",
            "486/486 - 4s - loss: 4.4544 - accuracy: 0.1669 - 4s/epoch - 9ms/step\n",
            "Epoch 34/300\n",
            "486/486 - 4s - loss: 4.4274 - accuracy: 0.1660 - 4s/epoch - 9ms/step\n",
            "Epoch 35/300\n",
            "486/486 - 4s - loss: 4.4001 - accuracy: 0.1715 - 4s/epoch - 9ms/step\n",
            "Epoch 36/300\n",
            "486/486 - 4s - loss: 4.3725 - accuracy: 0.1750 - 4s/epoch - 9ms/step\n",
            "Epoch 37/300\n",
            "486/486 - 5s - loss: 4.3534 - accuracy: 0.1739 - 5s/epoch - 10ms/step\n",
            "Epoch 38/300\n",
            "486/486 - 5s - loss: 4.3231 - accuracy: 0.1766 - 5s/epoch - 10ms/step\n",
            "Epoch 39/300\n",
            "486/486 - 4s - loss: 4.2962 - accuracy: 0.1791 - 4s/epoch - 9ms/step\n",
            "Epoch 40/300\n",
            "486/486 - 4s - loss: 4.2667 - accuracy: 0.1820 - 4s/epoch - 9ms/step\n",
            "Epoch 41/300\n",
            "486/486 - 4s - loss: 4.2464 - accuracy: 0.1856 - 4s/epoch - 9ms/step\n",
            "Epoch 42/300\n",
            "486/486 - 4s - loss: 4.2204 - accuracy: 0.1881 - 4s/epoch - 9ms/step\n",
            "Epoch 43/300\n",
            "486/486 - 4s - loss: 4.1962 - accuracy: 0.1904 - 4s/epoch - 9ms/step\n",
            "Epoch 44/300\n",
            "486/486 - 4s - loss: 4.1723 - accuracy: 0.1959 - 4s/epoch - 9ms/step\n",
            "Epoch 45/300\n",
            "486/486 - 4s - loss: 4.1422 - accuracy: 0.2018 - 4s/epoch - 9ms/step\n",
            "Epoch 46/300\n",
            "486/486 - 4s - loss: 4.1190 - accuracy: 0.2044 - 4s/epoch - 9ms/step\n",
            "Epoch 47/300\n",
            "486/486 - 4s - loss: 4.1026 - accuracy: 0.2054 - 4s/epoch - 9ms/step\n",
            "Epoch 48/300\n",
            "486/486 - 4s - loss: 4.0677 - accuracy: 0.2115 - 4s/epoch - 9ms/step\n",
            "Epoch 49/300\n",
            "486/486 - 4s - loss: 4.0391 - accuracy: 0.2169 - 4s/epoch - 9ms/step\n",
            "Epoch 50/300\n",
            "486/486 - 5s - loss: 4.0097 - accuracy: 0.2168 - 5s/epoch - 9ms/step\n",
            "Epoch 51/300\n",
            "486/486 - 5s - loss: 3.9793 - accuracy: 0.2214 - 5s/epoch - 9ms/step\n",
            "Epoch 52/300\n",
            "486/486 - 4s - loss: 3.9542 - accuracy: 0.2216 - 4s/epoch - 9ms/step\n",
            "Epoch 53/300\n",
            "486/486 - 4s - loss: 3.9348 - accuracy: 0.2286 - 4s/epoch - 9ms/step\n",
            "Epoch 54/300\n",
            "486/486 - 5s - loss: 3.8999 - accuracy: 0.2310 - 5s/epoch - 9ms/step\n",
            "Epoch 55/300\n",
            "486/486 - 4s - loss: 3.8934 - accuracy: 0.2357 - 4s/epoch - 9ms/step\n",
            "Epoch 56/300\n",
            "486/486 - 4s - loss: 3.8678 - accuracy: 0.2362 - 4s/epoch - 9ms/step\n",
            "Epoch 57/300\n",
            "486/486 - 4s - loss: 3.8301 - accuracy: 0.2417 - 4s/epoch - 9ms/step\n",
            "Epoch 58/300\n",
            "486/486 - 4s - loss: 3.8032 - accuracy: 0.2442 - 4s/epoch - 9ms/step\n",
            "Epoch 59/300\n",
            "486/486 - 4s - loss: 3.7859 - accuracy: 0.2459 - 4s/epoch - 9ms/step\n",
            "Epoch 60/300\n",
            "486/486 - 4s - loss: 3.7537 - accuracy: 0.2467 - 4s/epoch - 9ms/step\n",
            "Epoch 61/300\n",
            "486/486 - 4s - loss: 3.7383 - accuracy: 0.2514 - 4s/epoch - 9ms/step\n",
            "Epoch 62/300\n",
            "486/486 - 4s - loss: 3.7205 - accuracy: 0.2526 - 4s/epoch - 9ms/step\n",
            "Epoch 63/300\n",
            "486/486 - 4s - loss: 3.6912 - accuracy: 0.2605 - 4s/epoch - 9ms/step\n",
            "Epoch 64/300\n",
            "486/486 - 4s - loss: 3.6668 - accuracy: 0.2623 - 4s/epoch - 9ms/step\n",
            "Epoch 65/300\n",
            "486/486 - 4s - loss: 3.6430 - accuracy: 0.2665 - 4s/epoch - 9ms/step\n",
            "Epoch 66/300\n",
            "486/486 - 4s - loss: 3.6257 - accuracy: 0.2667 - 4s/epoch - 9ms/step\n",
            "Epoch 67/300\n",
            "486/486 - 4s - loss: 3.5936 - accuracy: 0.2721 - 4s/epoch - 9ms/step\n",
            "Epoch 68/300\n",
            "486/486 - 4s - loss: 3.5750 - accuracy: 0.2758 - 4s/epoch - 9ms/step\n",
            "Epoch 69/300\n",
            "486/486 - 4s - loss: 3.5489 - accuracy: 0.2773 - 4s/epoch - 9ms/step\n",
            "Epoch 70/300\n",
            "486/486 - 5s - loss: 3.5424 - accuracy: 0.2779 - 5s/epoch - 9ms/step\n",
            "Epoch 71/300\n",
            "486/486 - 4s - loss: 3.5232 - accuracy: 0.2829 - 4s/epoch - 9ms/step\n",
            "Epoch 72/300\n",
            "486/486 - 4s - loss: 3.4990 - accuracy: 0.2869 - 4s/epoch - 9ms/step\n",
            "Epoch 73/300\n",
            "486/486 - 4s - loss: 3.4716 - accuracy: 0.2935 - 4s/epoch - 9ms/step\n",
            "Epoch 74/300\n",
            "486/486 - 4s - loss: 3.4610 - accuracy: 0.2892 - 4s/epoch - 9ms/step\n",
            "Epoch 75/300\n",
            "486/486 - 4s - loss: 3.4288 - accuracy: 0.2973 - 4s/epoch - 9ms/step\n",
            "Epoch 76/300\n",
            "486/486 - 5s - loss: 3.4125 - accuracy: 0.2974 - 5s/epoch - 10ms/step\n",
            "Epoch 77/300\n",
            "486/486 - 5s - loss: 3.3930 - accuracy: 0.3056 - 5s/epoch - 10ms/step\n",
            "Epoch 78/300\n",
            "486/486 - 5s - loss: 3.3859 - accuracy: 0.3034 - 5s/epoch - 9ms/step\n",
            "Epoch 79/300\n",
            "486/486 - 4s - loss: 3.3612 - accuracy: 0.3055 - 4s/epoch - 9ms/step\n",
            "Epoch 80/300\n",
            "486/486 - 4s - loss: 3.3409 - accuracy: 0.3147 - 4s/epoch - 9ms/step\n",
            "Epoch 81/300\n",
            "486/486 - 5s - loss: 3.3190 - accuracy: 0.3166 - 5s/epoch - 9ms/step\n",
            "Epoch 82/300\n",
            "486/486 - 4s - loss: 3.3082 - accuracy: 0.3166 - 4s/epoch - 9ms/step\n",
            "Epoch 83/300\n",
            "486/486 - 5s - loss: 3.2858 - accuracy: 0.3240 - 5s/epoch - 9ms/step\n",
            "Epoch 84/300\n",
            "486/486 - 5s - loss: 3.2601 - accuracy: 0.3292 - 5s/epoch - 9ms/step\n",
            "Epoch 85/300\n",
            "486/486 - 4s - loss: 3.2440 - accuracy: 0.3283 - 4s/epoch - 9ms/step\n",
            "Epoch 86/300\n",
            "486/486 - 4s - loss: 3.2229 - accuracy: 0.3337 - 4s/epoch - 9ms/step\n",
            "Epoch 87/300\n",
            "486/486 - 4s - loss: 3.2094 - accuracy: 0.3382 - 4s/epoch - 9ms/step\n",
            "Epoch 88/300\n",
            "486/486 - 4s - loss: 3.1893 - accuracy: 0.3445 - 4s/epoch - 9ms/step\n",
            "Epoch 89/300\n",
            "486/486 - 4s - loss: 3.1666 - accuracy: 0.3436 - 4s/epoch - 9ms/step\n",
            "Epoch 90/300\n",
            "486/486 - 4s - loss: 3.1483 - accuracy: 0.3478 - 4s/epoch - 9ms/step\n",
            "Epoch 91/300\n",
            "486/486 - 4s - loss: 3.1422 - accuracy: 0.3465 - 4s/epoch - 9ms/step\n",
            "Epoch 92/300\n",
            "486/486 - 4s - loss: 3.1142 - accuracy: 0.3552 - 4s/epoch - 9ms/step\n",
            "Epoch 93/300\n",
            "486/486 - 4s - loss: 3.1011 - accuracy: 0.3607 - 4s/epoch - 9ms/step\n",
            "Epoch 94/300\n",
            "486/486 - 4s - loss: 3.0857 - accuracy: 0.3616 - 4s/epoch - 9ms/step\n",
            "Epoch 95/300\n",
            "486/486 - 4s - loss: 3.0709 - accuracy: 0.3580 - 4s/epoch - 9ms/step\n",
            "Epoch 96/300\n",
            "486/486 - 4s - loss: 3.0699 - accuracy: 0.3617 - 4s/epoch - 9ms/step\n",
            "Epoch 97/300\n",
            "486/486 - 4s - loss: 3.0359 - accuracy: 0.3678 - 4s/epoch - 9ms/step\n",
            "Epoch 98/300\n",
            "486/486 - 4s - loss: 3.0109 - accuracy: 0.3740 - 4s/epoch - 9ms/step\n",
            "Epoch 99/300\n",
            "486/486 - 4s - loss: 2.9947 - accuracy: 0.3788 - 4s/epoch - 9ms/step\n",
            "Epoch 100/300\n",
            "486/486 - 4s - loss: 2.9920 - accuracy: 0.3779 - 4s/epoch - 9ms/step\n",
            "Epoch 101/300\n",
            "486/486 - 4s - loss: 2.9685 - accuracy: 0.3779 - 4s/epoch - 9ms/step\n",
            "Epoch 102/300\n",
            "486/486 - 4s - loss: 2.9479 - accuracy: 0.3867 - 4s/epoch - 9ms/step\n",
            "Epoch 103/300\n",
            "486/486 - 4s - loss: 2.9273 - accuracy: 0.3904 - 4s/epoch - 9ms/step\n",
            "Epoch 104/300\n",
            "486/486 - 4s - loss: 2.9191 - accuracy: 0.3930 - 4s/epoch - 9ms/step\n",
            "Epoch 105/300\n",
            "486/486 - 4s - loss: 2.9018 - accuracy: 0.3957 - 4s/epoch - 9ms/step\n",
            "Epoch 106/300\n",
            "486/486 - 4s - loss: 2.8810 - accuracy: 0.4002 - 4s/epoch - 9ms/step\n",
            "Epoch 107/300\n",
            "486/486 - 4s - loss: 2.8768 - accuracy: 0.3975 - 4s/epoch - 9ms/step\n",
            "Epoch 108/300\n",
            "486/486 - 4s - loss: 2.8575 - accuracy: 0.4046 - 4s/epoch - 9ms/step\n",
            "Epoch 109/300\n",
            "486/486 - 4s - loss: 2.8414 - accuracy: 0.4094 - 4s/epoch - 9ms/step\n",
            "Epoch 110/300\n",
            "486/486 - 4s - loss: 2.8198 - accuracy: 0.4114 - 4s/epoch - 9ms/step\n",
            "Epoch 111/300\n",
            "486/486 - 4s - loss: 2.8092 - accuracy: 0.4124 - 4s/epoch - 9ms/step\n",
            "Epoch 112/300\n",
            "486/486 - 4s - loss: 2.7802 - accuracy: 0.4192 - 4s/epoch - 9ms/step\n",
            "Epoch 113/300\n",
            "486/486 - 4s - loss: 2.7754 - accuracy: 0.4233 - 4s/epoch - 9ms/step\n",
            "Epoch 114/300\n",
            "486/486 - 4s - loss: 2.7612 - accuracy: 0.4201 - 4s/epoch - 9ms/step\n",
            "Epoch 115/300\n",
            "486/486 - 5s - loss: 2.7485 - accuracy: 0.4279 - 5s/epoch - 10ms/step\n",
            "Epoch 116/300\n",
            "486/486 - 5s - loss: 2.7225 - accuracy: 0.4289 - 5s/epoch - 10ms/step\n",
            "Epoch 117/300\n",
            "486/486 - 4s - loss: 2.7139 - accuracy: 0.4321 - 4s/epoch - 9ms/step\n",
            "Epoch 118/300\n",
            "486/486 - 4s - loss: 2.7022 - accuracy: 0.4394 - 4s/epoch - 9ms/step\n",
            "Epoch 119/300\n",
            "486/486 - 4s - loss: 2.6928 - accuracy: 0.4353 - 4s/epoch - 9ms/step\n",
            "Epoch 120/300\n",
            "486/486 - 4s - loss: 2.6693 - accuracy: 0.4424 - 4s/epoch - 9ms/step\n",
            "Epoch 121/300\n",
            "486/486 - 4s - loss: 2.6369 - accuracy: 0.4518 - 4s/epoch - 9ms/step\n",
            "Epoch 122/300\n",
            "486/486 - 4s - loss: 2.6343 - accuracy: 0.4520 - 4s/epoch - 9ms/step\n",
            "Epoch 123/300\n",
            "486/486 - 4s - loss: 2.6218 - accuracy: 0.4546 - 4s/epoch - 9ms/step\n",
            "Epoch 124/300\n",
            "486/486 - 4s - loss: 2.6004 - accuracy: 0.4572 - 4s/epoch - 9ms/step\n",
            "Epoch 125/300\n",
            "486/486 - 4s - loss: 2.5870 - accuracy: 0.4598 - 4s/epoch - 9ms/step\n",
            "Epoch 126/300\n",
            "486/486 - 4s - loss: 2.5761 - accuracy: 0.4647 - 4s/epoch - 9ms/step\n",
            "Epoch 127/300\n",
            "486/486 - 4s - loss: 2.5586 - accuracy: 0.4691 - 4s/epoch - 9ms/step\n",
            "Epoch 128/300\n",
            "486/486 - 4s - loss: 2.5396 - accuracy: 0.4693 - 4s/epoch - 9ms/step\n",
            "Epoch 129/300\n",
            "486/486 - 4s - loss: 2.5250 - accuracy: 0.4758 - 4s/epoch - 9ms/step\n",
            "Epoch 130/300\n",
            "486/486 - 4s - loss: 2.5250 - accuracy: 0.4730 - 4s/epoch - 9ms/step\n",
            "Epoch 131/300\n",
            "486/486 - 4s - loss: 2.5099 - accuracy: 0.4759 - 4s/epoch - 9ms/step\n",
            "Epoch 132/300\n",
            "486/486 - 4s - loss: 2.4794 - accuracy: 0.4826 - 4s/epoch - 9ms/step\n",
            "Epoch 133/300\n",
            "486/486 - 4s - loss: 2.4627 - accuracy: 0.4853 - 4s/epoch - 9ms/step\n",
            "Epoch 134/300\n",
            "486/486 - 4s - loss: 2.4532 - accuracy: 0.4917 - 4s/epoch - 9ms/step\n",
            "Epoch 135/300\n",
            "486/486 - 4s - loss: 2.4380 - accuracy: 0.4957 - 4s/epoch - 9ms/step\n",
            "Epoch 136/300\n",
            "486/486 - 4s - loss: 2.4253 - accuracy: 0.4958 - 4s/epoch - 9ms/step\n",
            "Epoch 137/300\n",
            "486/486 - 4s - loss: 2.4223 - accuracy: 0.4941 - 4s/epoch - 9ms/step\n",
            "Epoch 138/300\n",
            "486/486 - 4s - loss: 2.4035 - accuracy: 0.5005 - 4s/epoch - 9ms/step\n",
            "Epoch 139/300\n",
            "486/486 - 4s - loss: 2.3874 - accuracy: 0.5033 - 4s/epoch - 9ms/step\n",
            "Epoch 140/300\n",
            "486/486 - 4s - loss: 2.3974 - accuracy: 0.4998 - 4s/epoch - 9ms/step\n",
            "Epoch 141/300\n",
            "486/486 - 4s - loss: 2.3681 - accuracy: 0.5077 - 4s/epoch - 9ms/step\n",
            "Epoch 142/300\n",
            "486/486 - 4s - loss: 2.3584 - accuracy: 0.5102 - 4s/epoch - 9ms/step\n",
            "Epoch 143/300\n",
            "486/486 - 4s - loss: 2.3469 - accuracy: 0.5134 - 4s/epoch - 9ms/step\n",
            "Epoch 144/300\n",
            "486/486 - 4s - loss: 2.3351 - accuracy: 0.5095 - 4s/epoch - 9ms/step\n",
            "Epoch 145/300\n",
            "486/486 - 4s - loss: 2.3140 - accuracy: 0.5190 - 4s/epoch - 9ms/step\n",
            "Epoch 146/300\n",
            "486/486 - 4s - loss: 2.3006 - accuracy: 0.5208 - 4s/epoch - 9ms/step\n",
            "Epoch 147/300\n",
            "486/486 - 4s - loss: 2.2948 - accuracy: 0.5242 - 4s/epoch - 9ms/step\n",
            "Epoch 148/300\n",
            "486/486 - 4s - loss: 2.2746 - accuracy: 0.5289 - 4s/epoch - 9ms/step\n",
            "Epoch 149/300\n",
            "486/486 - 4s - loss: 2.2697 - accuracy: 0.5326 - 4s/epoch - 9ms/step\n",
            "Epoch 150/300\n",
            "486/486 - 4s - loss: 2.2656 - accuracy: 0.5279 - 4s/epoch - 9ms/step\n",
            "Epoch 151/300\n",
            "486/486 - 5s - loss: 2.2483 - accuracy: 0.5324 - 5s/epoch - 9ms/step\n",
            "Epoch 152/300\n",
            "486/486 - 5s - loss: 2.2466 - accuracy: 0.5349 - 5s/epoch - 9ms/step\n",
            "Epoch 153/300\n",
            "486/486 - 5s - loss: 2.2299 - accuracy: 0.5363 - 5s/epoch - 9ms/step\n",
            "Epoch 154/300\n",
            "486/486 - 5s - loss: 2.2151 - accuracy: 0.5379 - 5s/epoch - 11ms/step\n",
            "Epoch 155/300\n",
            "486/486 - 5s - loss: 2.1937 - accuracy: 0.5446 - 5s/epoch - 9ms/step\n",
            "Epoch 156/300\n",
            "486/486 - 4s - loss: 2.1938 - accuracy: 0.5484 - 4s/epoch - 9ms/step\n",
            "Epoch 157/300\n",
            "486/486 - 4s - loss: 2.1698 - accuracy: 0.5507 - 4s/epoch - 9ms/step\n",
            "Epoch 158/300\n",
            "486/486 - 4s - loss: 2.1569 - accuracy: 0.5530 - 4s/epoch - 9ms/step\n",
            "Epoch 159/300\n",
            "486/486 - 5s - loss: 2.1664 - accuracy: 0.5519 - 5s/epoch - 9ms/step\n",
            "Epoch 160/300\n",
            "486/486 - 4s - loss: 2.1484 - accuracy: 0.5542 - 4s/epoch - 9ms/step\n",
            "Epoch 161/300\n",
            "486/486 - 4s - loss: 2.1467 - accuracy: 0.5524 - 4s/epoch - 9ms/step\n",
            "Epoch 162/300\n",
            "486/486 - 4s - loss: 2.1241 - accuracy: 0.5603 - 4s/epoch - 9ms/step\n",
            "Epoch 163/300\n",
            "486/486 - 4s - loss: 2.1151 - accuracy: 0.5620 - 4s/epoch - 9ms/step\n",
            "Epoch 164/300\n",
            "486/486 - 4s - loss: 2.1054 - accuracy: 0.5642 - 4s/epoch - 9ms/step\n",
            "Epoch 165/300\n",
            "486/486 - 4s - loss: 2.0881 - accuracy: 0.5677 - 4s/epoch - 9ms/step\n",
            "Epoch 166/300\n",
            "486/486 - 4s - loss: 2.0808 - accuracy: 0.5716 - 4s/epoch - 9ms/step\n",
            "Epoch 167/300\n",
            "486/486 - 4s - loss: 2.0792 - accuracy: 0.5686 - 4s/epoch - 9ms/step\n",
            "Epoch 168/300\n",
            "486/486 - 4s - loss: 2.0648 - accuracy: 0.5741 - 4s/epoch - 9ms/step\n",
            "Epoch 169/300\n",
            "486/486 - 4s - loss: 2.0543 - accuracy: 0.5777 - 4s/epoch - 9ms/step\n",
            "Epoch 170/300\n",
            "486/486 - 4s - loss: 2.0479 - accuracy: 0.5795 - 4s/epoch - 9ms/step\n",
            "Epoch 171/300\n",
            "486/486 - 4s - loss: 2.0392 - accuracy: 0.5785 - 4s/epoch - 9ms/step\n",
            "Epoch 172/300\n",
            "486/486 - 4s - loss: 2.0292 - accuracy: 0.5803 - 4s/epoch - 9ms/step\n",
            "Epoch 173/300\n",
            "486/486 - 4s - loss: 2.0084 - accuracy: 0.5872 - 4s/epoch - 9ms/step\n",
            "Epoch 174/300\n",
            "486/486 - 4s - loss: 2.0020 - accuracy: 0.5876 - 4s/epoch - 9ms/step\n",
            "Epoch 175/300\n",
            "486/486 - 4s - loss: 1.9891 - accuracy: 0.5864 - 4s/epoch - 9ms/step\n",
            "Epoch 176/300\n",
            "486/486 - 4s - loss: 1.9888 - accuracy: 0.5880 - 4s/epoch - 9ms/step\n",
            "Epoch 177/300\n",
            "486/486 - 4s - loss: 1.9711 - accuracy: 0.5915 - 4s/epoch - 9ms/step\n",
            "Epoch 178/300\n",
            "486/486 - 4s - loss: 1.9602 - accuracy: 0.5929 - 4s/epoch - 9ms/step\n",
            "Epoch 179/300\n",
            "486/486 - 4s - loss: 1.9646 - accuracy: 0.5929 - 4s/epoch - 9ms/step\n",
            "Epoch 180/300\n",
            "486/486 - 4s - loss: 1.9692 - accuracy: 0.5919 - 4s/epoch - 9ms/step\n",
            "Epoch 181/300\n",
            "486/486 - 4s - loss: 1.9603 - accuracy: 0.5939 - 4s/epoch - 9ms/step\n",
            "Epoch 182/300\n",
            "486/486 - 4s - loss: 1.9384 - accuracy: 0.6012 - 4s/epoch - 9ms/step\n",
            "Epoch 183/300\n",
            "486/486 - 4s - loss: 1.9199 - accuracy: 0.6003 - 4s/epoch - 9ms/step\n",
            "Epoch 184/300\n",
            "486/486 - 4s - loss: 1.9006 - accuracy: 0.6083 - 4s/epoch - 9ms/step\n",
            "Epoch 185/300\n",
            "486/486 - 4s - loss: 1.9020 - accuracy: 0.6075 - 4s/epoch - 9ms/step\n",
            "Epoch 186/300\n",
            "486/486 - 4s - loss: 1.9119 - accuracy: 0.6081 - 4s/epoch - 9ms/step\n",
            "Epoch 187/300\n",
            "486/486 - 5s - loss: 1.8955 - accuracy: 0.6072 - 5s/epoch - 9ms/step\n",
            "Epoch 188/300\n",
            "486/486 - 4s - loss: 1.8696 - accuracy: 0.6169 - 4s/epoch - 9ms/step\n",
            "Epoch 189/300\n",
            "486/486 - 4s - loss: 1.8873 - accuracy: 0.6093 - 4s/epoch - 9ms/step\n",
            "Epoch 190/300\n",
            "486/486 - 4s - loss: 1.8658 - accuracy: 0.6147 - 4s/epoch - 9ms/step\n",
            "Epoch 191/300\n",
            "486/486 - 4s - loss: 1.8627 - accuracy: 0.6140 - 4s/epoch - 9ms/step\n",
            "Epoch 192/300\n",
            "486/486 - 4s - loss: 1.8377 - accuracy: 0.6198 - 4s/epoch - 9ms/step\n",
            "Epoch 193/300\n",
            "486/486 - 5s - loss: 1.8391 - accuracy: 0.6201 - 5s/epoch - 9ms/step\n",
            "Epoch 194/300\n",
            "486/486 - 5s - loss: 1.8296 - accuracy: 0.6231 - 5s/epoch - 10ms/step\n",
            "Epoch 195/300\n",
            "486/486 - 4s - loss: 1.8359 - accuracy: 0.6237 - 4s/epoch - 9ms/step\n",
            "Epoch 196/300\n",
            "486/486 - 5s - loss: 1.8241 - accuracy: 0.6229 - 5s/epoch - 9ms/step\n",
            "Epoch 197/300\n",
            "486/486 - 4s - loss: 1.8063 - accuracy: 0.6279 - 4s/epoch - 9ms/step\n",
            "Epoch 198/300\n",
            "486/486 - 4s - loss: 1.7983 - accuracy: 0.6295 - 4s/epoch - 9ms/step\n",
            "Epoch 199/300\n",
            "486/486 - 5s - loss: 1.7837 - accuracy: 0.6318 - 5s/epoch - 9ms/step\n",
            "Epoch 200/300\n",
            "486/486 - 4s - loss: 1.7766 - accuracy: 0.6356 - 4s/epoch - 9ms/step\n",
            "Epoch 201/300\n",
            "486/486 - 4s - loss: 1.7787 - accuracy: 0.6336 - 4s/epoch - 9ms/step\n",
            "Epoch 202/300\n",
            "486/486 - 4s - loss: 1.7697 - accuracy: 0.6348 - 4s/epoch - 9ms/step\n",
            "Epoch 203/300\n",
            "486/486 - 4s - loss: 1.7528 - accuracy: 0.6392 - 4s/epoch - 9ms/step\n",
            "Epoch 204/300\n",
            "486/486 - 4s - loss: 1.7611 - accuracy: 0.6367 - 4s/epoch - 9ms/step\n",
            "Epoch 205/300\n",
            "486/486 - 4s - loss: 1.7594 - accuracy: 0.6360 - 4s/epoch - 9ms/step\n",
            "Epoch 206/300\n",
            "486/486 - 4s - loss: 1.7519 - accuracy: 0.6423 - 4s/epoch - 9ms/step\n",
            "Epoch 207/300\n",
            "486/486 - 4s - loss: 1.7389 - accuracy: 0.6430 - 4s/epoch - 9ms/step\n",
            "Epoch 208/300\n",
            "486/486 - 4s - loss: 1.7258 - accuracy: 0.6413 - 4s/epoch - 9ms/step\n",
            "Epoch 209/300\n",
            "486/486 - 4s - loss: 1.7180 - accuracy: 0.6451 - 4s/epoch - 9ms/step\n",
            "Epoch 210/300\n",
            "486/486 - 4s - loss: 1.7146 - accuracy: 0.6482 - 4s/epoch - 9ms/step\n",
            "Epoch 211/300\n",
            "486/486 - 4s - loss: 1.7154 - accuracy: 0.6474 - 4s/epoch - 9ms/step\n",
            "Epoch 212/300\n",
            "486/486 - 4s - loss: 1.6977 - accuracy: 0.6526 - 4s/epoch - 9ms/step\n",
            "Epoch 213/300\n",
            "486/486 - 4s - loss: 1.7073 - accuracy: 0.6472 - 4s/epoch - 9ms/step\n",
            "Epoch 214/300\n",
            "486/486 - 4s - loss: 1.7104 - accuracy: 0.6477 - 4s/epoch - 9ms/step\n",
            "Epoch 215/300\n",
            "486/486 - 4s - loss: 1.6870 - accuracy: 0.6546 - 4s/epoch - 9ms/step\n",
            "Epoch 216/300\n",
            "486/486 - 4s - loss: 1.6729 - accuracy: 0.6570 - 4s/epoch - 9ms/step\n",
            "Epoch 217/300\n",
            "486/486 - 4s - loss: 1.6461 - accuracy: 0.6638 - 4s/epoch - 9ms/step\n",
            "Epoch 218/300\n",
            "486/486 - 4s - loss: 1.6583 - accuracy: 0.6576 - 4s/epoch - 9ms/step\n",
            "Epoch 219/300\n",
            "486/486 - 4s - loss: 1.6565 - accuracy: 0.6571 - 4s/epoch - 9ms/step\n",
            "Epoch 220/300\n",
            "486/486 - 4s - loss: 1.6533 - accuracy: 0.6584 - 4s/epoch - 9ms/step\n",
            "Epoch 221/300\n",
            "486/486 - 4s - loss: 1.6445 - accuracy: 0.6625 - 4s/epoch - 9ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33632ee310>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#다중 클래스 분류 모델 만들기 (마지막 시점에서 모든 가능한 단어 중 하나의 단어를 예측)\n",
        "#하이퍼파라미터 - 임베딩 벡터의 차원은 15, 은닉 상태의 크기는 128\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "# model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(100))\n",
        "# model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(Dense(total_words, activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# print(model.summary())\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'accuracy', patience = 4, mode = 'max')\n",
        "\n",
        "embedding_dim = 15\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(vocab_size/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=300, verbose=2, callbacks = [early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Zhips9k9ATpo"
      },
      "outputs": [],
      "source": [
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for _ in range(n):\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n",
        "\n",
        "        # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJrEk0r_GjAp",
        "outputId": "aeeb119f-61ab-43f6-b7e5-82f15be2897f"
      },
      "outputs": [],
      "source": [
        "tokenizer_keras.word_index.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFGg596ZEM2G",
        "outputId": "c6351744-a97b-4af7-da65-5e2a5bd85397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "눈물 의 위선 에 목 졸린 채 로 바다 만들 어 고 있 어 너 의 기술 과 잠든 이러 다 잠든 하 겠 줘 날 이젠 조금 더 그리워 하 될지 고 야 맘 의 연속 길 바랬 지 한없이 더러워 길 바라 겠 죠 적어도 너 와 어디 있\n"
          ]
        }
      ],
      "source": [
        "#임의의 단어로 시작하는 문장 생성\n",
        "\n",
        "print(sentence_generation(model, tokenizer_keras, '눈물', 50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LXbgh4unEsn",
        "outputId": "ee378ccf-c8cc-4346-82f2-10ac03a959c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "꿈 꾸 렴 서부터 바라보 고 있 거 맞 죠 우리 쯤 에 있 어 내 마음 줘 내 손안 에 살 아 한단 모습 해 주 고 있 어 너 의 몰상식 함 에 필요 해 질 그 개 몇 개 의 아름다움 에 꼬리 어 모든 달 든 이렇\n"
          ]
        }
      ],
      "source": [
        "print(sentence_generation(model, tokenizer_keras, '꿈', 50))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
